{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8a4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Alle ben√∂tigten Module sind vorhanden.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'model_benchmark_utils' from 'c:\\\\Users\\\\Marc\\\\Desktop\\\\benchmark\\\\stonks\\\\project\\\\DeepLearningProject\\\\Benchmark\\\\model_benchmark_utils.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === IMPORTS ===\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import importlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add current directory to Python path\n",
    "current_dir = os.path.dirname(os.path.abspath('__file__'))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.append(current_dir)\n",
    "    print(f\"‚úÖ Current directory added to PYTHONPATH: {current_dir}\")\n",
    "\n",
    "# Check if all external modules are present\n",
    "required_modules = ['ollama_server', 'model_manager', 'benchmark_core', 'visualization', 'model_benchmark_utils']\n",
    "missing_modules = []\n",
    "\n",
    "for module in required_modules:\n",
    "    try:\n",
    "        importlib.import_module(module)\n",
    "    except ImportError:\n",
    "        missing_modules.append(module)\n",
    "\n",
    "if missing_modules:\n",
    "    print(f\"\\n‚ùå Missing modules: {', '.join(missing_modules)}\")\n",
    "    print(\"Make sure all required Python modules are present in the same directory as this notebook.\")\n",
    "    # If modules are missing, inform the user but don't abort\n",
    "else:\n",
    "    print(\"‚úÖ All required modules are present.\")\n",
    "\n",
    "# Import functions from external modules\n",
    "from ollama_server import check_ollama_server, start_ollama_server\n",
    "from model_manager import check_model_exists, load_model\n",
    "from benchmark_core import benchmark_model, run_benchmark\n",
    "from visualization import visualize_results\n",
    "from model_benchmark_utils import run_benchmark_test\n",
    "\n",
    "# Optional: Reload modules to ensure the latest versions are used\n",
    "importlib.reload(importlib.import_module('ollama_server'))\n",
    "importlib.reload(importlib.import_module('model_manager'))\n",
    "importlib.reload(importlib.import_module('benchmark_core'))\n",
    "importlib.reload(importlib.import_module('visualization'))\n",
    "importlib.reload(importlib.import_module('model_benchmark_utils'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76177198",
   "metadata": {},
   "source": [
    "# LLM Benchmark Framework\n",
    "\n",
    "This notebook enables the comparison of arbitrary language models accessible through Ollama. You can use it to compare different models in terms of speed, performance and response quality for custom tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9953afd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === BENCHMARK CONFIGURATION ===\n",
    "# Ollama API URL\n",
    "OLLAMA_API_URL = \"http://localhost:11434/api\"\n",
    "\n",
    "# Models to compare (specify two or more models)\n",
    "MODELS = [\n",
    "    \"deepseek-r1:1.5b\",  # DeepSeek R1 with 1.5 billion parameters\n",
    "    \"llama3.2\"        # Llama 3.2 with 8 billion parameters\n",
    "]\n",
    "\n",
    "# Additional model suggestions:\n",
    "# - \"phi3:3.8b\" - Microsoft Phi-3 (small, efficient model) - 3.8 billion parameters\n",
    "# - \"mistral:7b\" - Mistral 7B (good balance between size and performance) - 7 billion parameters\n",
    "# - \"gemma:7b\" - Google Gemma 7B (efficient open-source model) - 7 billion parameters\n",
    "# - \"codellama:7b\" - Code Llama 7B (specialized for code generation) - 7 billion parameters\n",
    "# - \"llama3:8b\" - Meta Llama 3 8B (larger base model) - 8 billion parameters\n",
    "# - \"qwen2:7b\" - Qwen2 7B (multilingual model with good German support) - 7 billion parameters\n",
    "# - \"neural-chat:7b\" - Neural Chat 7B (optimized for conversations) - 7 billion parameters\n",
    "# - \"wizardcoder:7b\" - WizardCoder 7B (good for programming tasks) - 7 billion parameters\n",
    "\n",
    "# Tasks for the benchmark (can be extended as needed)\n",
    "BENCHMARK_TASKS = [\n",
    "    {\n",
    "        \"name\": \"Text Generation\",\n",
    "        \"prompt\": \"Write a short paragraph about AI.\",\n",
    "        \"max_tokens\": 30,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Code Generation\",\n",
    "        \"prompt\": \"Write a simple Python function that checks if a number is even.\",\n",
    "        \"max_tokens\": 30,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Factual Knowledge\",\n",
    "        \"prompt\": \"What is the difference between ML and AI? Short answer.\",\n",
    "        \"max_tokens\": 30,\n",
    "    }\n",
    "]\n",
    "\n",
    "# Benchmark parameters\n",
    "REQUEST_TIMEOUT = 120  # Timeout for model responses (seconds)\n",
    "RETRY_TIMEOUT = 300    # Timeout for retry attempts (seconds)\n",
    "TEMPERATURE = 0.0      # Sampling temperature for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f540e3",
   "metadata": {},
   "source": [
    "## Running the Benchmark\n",
    "\n",
    "Starting the benchmark test and displaying the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e571c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starte Benchmark-Test f√ºr deepseek-r1:1.5b, llama3.2...\n",
      "\n",
      "üöÄ Versuche den Ollama-Server zu starten...\n",
      "‚ùå Fehler beim Starten des Ollama-Servers: [WinError 2] The system cannot find the file specified\n",
      "‚ÑπÔ∏è Bitte starten Sie den Ollama-Server manuell mit dem Befehl 'ollama serve'\n",
      "‚ùå Ollama-Server konnte nicht gestartet werden.\n",
      "\n",
      "‚ùå Keine Ergebnisse zum Visualisieren oder Speichern vorhanden.\n"
     ]
    }
   ],
   "source": [
    "# Execute the benchmark \n",
    "# In the notebook only call to external function - no definition\n",
    "\n",
    "# Run benchmark with the configured parameters\n",
    "benchmark_results = run_benchmark_test(\n",
    "    api_url=OLLAMA_API_URL,\n",
    "    models=MODELS,\n",
    "    tasks=BENCHMARK_TASKS,\n",
    "    temperature=TEMPERATURE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
